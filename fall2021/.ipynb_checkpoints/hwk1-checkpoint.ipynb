{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35d9c630-76df-40a4-b436-e21f429c7cac",
   "metadata": {},
   "source": [
    "### Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76d6cd5-1df1-40ee-aa8e-49b8b8b55873",
   "metadata": {},
   "source": [
    "##### 1.  Write a function load_data which loads the data, preprocesses it using a vectorizer (http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_ extraction.text, we suggest you use CountVectorizer as it is the simplest in nature), and splits the entire dataset randomly into 70% training, 15% validation, and 15% test examples. You may use train_test_split function of scikit-learn within this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5681869d-38ac-40d2-8291-ae3addb7b22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "526aeaa7-d086-44cc-8b1e-33292cdeb043",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFake = pd.read_csv(\"/Users/gabriel/python/csc2515/fall2021/clean_fake.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4dec922-fd14-48fa-8b4b-13e146d8178b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfReal = pd.read_csv(\"/Users/gabriel/python/csc2515/fall2021/clean_real.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab16f89-b295-46c4-b829-cfe136be5f2b",
   "metadata": {},
   "source": [
    "##### Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56a779ba-639b-4e98-8b46-06c6e1b4a170",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_PATH='/Users/gabriel/python/csc2515/fall2021/hwk1/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd7cf9ea-4f2f-4abe-928f-84ea524baac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['fake', 'real']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2e384d8-19f3-4ce2-b0b0-d4d508504aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_DS_Store_files(categories, projectPath):\n",
    "    for category in categories:\n",
    "        ds_store_file_location = projectPath'+category+'/.DS_store'\n",
    "        if os.path.isfile(ds_store_file_location):\n",
    "            os.remove(ds_store_file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88832cdd-7a9d-44d9-9d31-699d1d628647",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_to_train = sklearn.datasets.load_files(\"/Users/gabriel/python/csc2515/fall2021/hwk1/\", \n",
    "                                            description=None, categories=categories, load_content=True, \n",
    "                                            shuffle=True, encoding='utf-8', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5e1e9d9b-3a1f-4ad9-83f8-b9b80513b7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fake', 'real']\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\"\n",
    "Question (a)\n",
    "----\n",
    "\n",
    "Loads data, preprocesses it using CountVectorizer and split the dataset into train, validation and\n",
    "test sets.\n",
    "\n",
    "\"\"\"\"\"\n",
    "def load_data(projectPath):\n",
    "    # Deletes .DS_Stores files, Bug in MacOS for native load_files sklearn method\n",
    "    delete_DS_Store_files(categories, projectPath)\n",
    "    \n",
    "    # Load data using native sklearn.datasets.load_files, it reads a list of categories and creates\n",
    "    # both the target and the categories automatically\n",
    "    docs = sklearn.datasets.load_files(projectPath, description=None, categories=categories, load_content=True, \n",
    "                                            shuffle=True, encoding='utf-8', random_state=42)\n",
    "    \n",
    "    # Dividing the dataset in Train, Validation and Test sets\n",
    "    #X_train, X_tmp, y_train, y_tmp = train_test_split(docs.data, docs.target, test_size=0.3)\n",
    "    #X_valid, X_test, y_valid, y_test = train_test_split(X_tmp, y_tmp, test_size=0.5)\n",
    "    #print(\"Documents in training set: \"+str(len(X_train)))\n",
    "    #print(\"Documents in validation set: \"+len(X_valid.data))\n",
    "    #print(\"Documents in test set: \"+len(X_test.data))\n",
    "    \n",
    "\n",
    "    print(docs.target_names)\n",
    "    print(len(docs.data))\n",
    "    print(len(docs.filenames))\n",
    "    \n",
    "    count_vect = CountVectorizer()\n",
    "    X_train_counts = count_vect.fit_transform(docs.data)\n",
    "    \n",
    "    \n",
    "load_data(PROJECT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a85677-68ef-4ac8-aeac-43824f1344b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
